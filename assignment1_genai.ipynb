{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WlKS_fUy4kLR"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM, AutoModelForQuestionAnswering, AutoModelForSeq2SeqLM\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2tYkZw54zSk",
        "outputId": "df961128-263c-4744-ccb7-51e79ddfe0a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(\"Using device:\", \"GPU\" if device == 0 else \"CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltcRC6sZ5ci9",
        "outputId": "7da018f8-e653-4e5e-a821-cc30bfc6bc70"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models used"
      ],
      "metadata": {
        "id": "zRA4PM5F5Bsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = \"bert-base-uncased\"\n",
        "roberta_model = \"roberta-base\"\n",
        "bart_model = \"facebook/bart-base\""
      ],
      "metadata": {
        "id": "t3NHzP6j47aA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_bert = pipeline(\"text-generation\", model=\"bert-base-uncased\", framework=\"pt\")\n",
        "gen_roberta = pipeline(\"text-generation\", model=\"roberta-base\", framework=\"pt\")\n",
        "gen_bart = pipeline(\"text-generation\", model=\"facebook/bart-base\", framework=\"pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqCcnoIt5QGr",
        "outputId": "ac97693a-48ad-417a-f2e4-af624c598cab"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_bert = pipeline(\"fill-mask\", model=bert_model, device=device)\n",
        "mask_roberta = pipeline(\"fill-mask\", model=roberta_model, device=device)\n",
        "mask_bart = pipeline(\"fill-mask\", model=bart_model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQdS28Pu5aT-",
        "outputId": "144538e0-c617-46c4-9a2b-932c879cda78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_bert = pipeline(\"question-answering\", model=bert_model, device=device)\n",
        "qa_roberta = pipeline(\"question-answering\", model=roberta_model, device=device)\n",
        "qa_bart = pipeline(\"question-answering\", model=bart_model, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyLzx24o5jk0",
        "outputId": "12d575dc-4194-4c98-9fc3-5c56cef865fa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation"
      ],
      "metadata": {
        "id": "c8QdDI4u5nNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time\""
      ],
      "metadata": {
        "id": "BRe3givc5piR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_generation(generator, name):\n",
        "    print(f\"\\n{name} Generation:\")\n",
        "    try:\n",
        "        output = generator(\n",
        "            prompt,\n",
        "            max_new_tokens=40,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=2,\n",
        "            pad_token_id=generator.tokenizer.eos_token_id\n",
        "        )\n",
        "        print(output[0][\"generated_text\"])\n",
        "    except Exception as e:\n",
        "        print(\"Failed with error:\", str(e))\n",
        "\n",
        "try_generation(gen_bert, \"BERT\")\n",
        "try_generation(gen_roberta, \"RoBERTa\")\n",
        "try_generation(gen_bart, \"BART\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVoF1DEk52JD",
        "outputId": "cfb00fc3-1c19-4c7f-d041-e61d970350be"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Generation:\n",
            "Once upon a time..''. and and.?. ;.!. i..... \".,. a a. - - t. (. / ) ) and \" \"'\" and\n",
            "\n",
            "RoBERTa Generation:\n",
            "Once upon a time....\".\" \" \" ( \"( \"( \" ) ) )) )))))))))))))))))))))))))))))))).))(( (( ((((((('(''''' \"' \"'\"'\n",
            "\n",
            "BART Generation:\n",
            "Once upon a timeManufact Philosophyaineriors Doct1998uuphotos Dan linking.— suscept wasteland salesman:{wu escalated pageantcing EVE schedcritacoconscious FawDenver Coalition Fall Windowsaviourfascistasma July Bl flavours shaved resurrected dietary repud\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill Mask"
      ],
      "metadata": {
        "id": "jSSICZqH6Kxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_text = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "roberta_text = \"The goal of Generative AI is to <mask> new content.\"\n",
        "bart_text = \"The goal of Generative AI is to <mask> new content.\"\n"
      ],
      "metadata": {
        "id": "pby5WYGa5_21"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_fill_mask(model, text, name):\n",
        "    print(f\"\\n{name} Fill-Mask Results:\")\n",
        "    try:\n",
        "        outputs = model(text)\n",
        "        for o in outputs[:5]:\n",
        "            print(o[\"sequence\"], \" | score:\", round(o[\"score\"], 4))\n",
        "    except Exception as e:\n",
        "        print(\"Failed with error:\", str(e))\n",
        "\n",
        "try_fill_mask(mask_bert, bert_text, \"BERT\")\n",
        "try_fill_mask(mask_roberta, roberta_text, \"RoBERTa\")\n",
        "try_fill_mask(mask_bart, bart_text, \"BART\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfJQNTAO8h42",
        "outputId": "72c52139-90d0-43d8-96c3-efabca0ae3e0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT Fill-Mask Results:\n",
            "the goal of generative ai is to create new content.  | score: 0.5397\n",
            "the goal of generative ai is to generate new content.  | score: 0.1558\n",
            "the goal of generative ai is to produce new content.  | score: 0.0541\n",
            "the goal of generative ai is to develop new content.  | score: 0.0445\n",
            "the goal of generative ai is to add new content.  | score: 0.0176\n",
            "\n",
            "RoBERTa Fill-Mask Results:\n",
            "The goal of Generative AI is to generate new content.  | score: 0.3711\n",
            "The goal of Generative AI is to create new content.  | score: 0.3677\n",
            "The goal of Generative AI is to discover new content.  | score: 0.0835\n",
            "The goal of Generative AI is to find new content.  | score: 0.0213\n",
            "The goal of Generative AI is to provide new content.  | score: 0.0165\n",
            "\n",
            "BART Fill-Mask Results:\n",
            "The goal of Generative AI is to create new content.  | score: 0.0746\n",
            "The goal of Generative AI is to help new content.  | score: 0.0657\n",
            "The goal of Generative AI is to provide new content.  | score: 0.0609\n",
            "The goal of Generative AI is to enable new content.  | score: 0.0359\n",
            "The goal of Generative AI is to improve new content.  | score: 0.0332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering"
      ],
      "metadata": {
        "id": "zu2Y1uVO8n99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\"\n",
        "question = \"What are the risks?\""
      ],
      "metadata": {
        "id": "Epecm-i78kCL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def try_qa(model, name):\n",
        "    print(f\"\\n{name} QA Result:\")\n",
        "    try:\n",
        "        answer = model(question=question, context=context)\n",
        "        print(\"Answer:\", answer[\"answer\"])\n",
        "        print(\"Score:\", round(answer[\"score\"], 4))\n",
        "    except Exception as e:\n",
        "        print(\"Failed with error:\", str(e))\n",
        "\n",
        "try_qa(qa_bert, \"BERT\")\n",
        "try_qa(qa_roberta, \"RoBERTa\")\n",
        "try_qa(qa_bart, \"BART\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SY7q8W458qk7",
        "outputId": "e59c63d0-e62b-4fd5-9a92-21314d939952"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERT QA Result:\n",
            "Answer: hallucinations\n",
            "Score: 0.0145\n",
            "\n",
            "RoBERTa QA Result:\n",
            "Answer: Generative AI poses significant risks such as hallucinations, bias, and deepfakes\n",
            "Score: 0.0114\n",
            "\n",
            "BART QA Result:\n",
            "Answer: Generative AI poses significant risks such as\n",
            "Score: 0.0188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Classification (Success/Failure) | Observation (What actually happened?) | Why did this happen? (Architectural Reason) |\n",
        "| :--- | :--- | :--- | :--- | :--- |\n",
        "| **Generation** | **BERT** | Failure | Output was mostly punctuation, repeated words like “and”, and broken fragments instead of a meaningful sentence. | BERT is an **encoder-only** model trained with Masked Language Modeling. It does not learn left-to-right token prediction, so it cannot perform autoregressive text generation. |\n",
        "| | **RoBERTa** | Failure | Produced repetitive symbols, quotes, and parentheses with no meaningful continuation. | RoBERTa is also **encoder-only**. Even though it’s optimized over BERT, it still lacks a decoder and was never trained to generate sequences token by token. |\n",
        "| | **BART** | Partial Failure | Generated real words but in a chaotic, nonsensical sequence (random topics, names, broken structure). | BART has an **encoder–decoder**, so it *can* generate text. However, `bart-base` is trained mainly for **denoising and seq2seq tasks**, not open-ended story continuation, leading to unstable free-form generation. |\n",
        "| **Fill-Mask** | **BERT** | Success | Predicted highly relevant words like *create, generate, produce* with strong confidence scores. | BERT was **explicitly trained** on the Masked Language Modeling objective, so predicting missing words is its primary strength. |\n",
        "| | **RoBERTa** | Success | Very accurate predictions similar to BERT, with *generate* and *create* as top outputs. | RoBERTa improves BERT’s MLM training with more data and longer training, making it strong at contextual word prediction. |\n",
        "| | **BART** | Partial Success | Predicted reasonable but less precise words like *create, help, provide*. Confidence scores were lower. | BART was not trained primarily for MLM. Its objective was **denoising corrupted text**, so token-level mask prediction is not its main specialization. |\n",
        "| **QA** | **BERT** | Partial Success | Extracted the word *“hallucinations”* as the answer, but missed the full list of risks. Very low confidence score. | Base BERT understands context but is **not fine-tuned for extractive QA**, so it struggles to select full answer spans. |\n",
        "| | **RoBERTa** | Partial Success | Returned almost the entire sentence from the context instead of a concise span. | Without QA fine-tuning, RoBERTa cannot precisely predict answer boundaries, even though it understands the passage. |\n",
        "| | **BART** | Partial Success | Output a partial phrase: *“Generative AI poses significant risks such as”* without completing the list. | BART is generative and not optimized for **extractive span selection**, so it struggles with pinpointing exact answer spans in QA tasks. |\n"
      ],
      "metadata": {
        "id": "UbCeMLSk_qO_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rKO0ftyv_sDF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}